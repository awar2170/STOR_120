{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6851754",
   "metadata": {},
   "source": [
    "# STOR 120: Take Home Midterm 1\n",
    "\n",
    "60 points total\n",
    "\n",
    "**Due:** Wednesday, February 9th to Gradescope before the start of class time.\n",
    "\n",
    "    Section 001: 12:20pm\n",
    "    Section 002:  1:25pm\n",
    "  \n",
    "**Directions:** The exam is open book, notes, course materials, internet, and all things that are not direct communication with others. Just as with all course assignments, you will submit exams to Gradescope as Jupyter Notebooks with the ipynb file extension. To receive full credit, you should show all of your code used to answer each question. Make sure to view your submission in Gradescope and verify that it is the correct file and has the format that you intended it to have, including all code being shown and run.\n",
    "\n",
    "Come to office hours if you have specific questions regarding the exam. Due to the large class sizes, individual questions sent via email are not possible to answer for all students. Please refrain from posting public questions to Piazza before the exam is due.  \n",
    "\n",
    "**Data:** The dataset used on this exam contains an overview of the 50 most crowded airports in the world for each year from 2016 to 2020. Each row in the table is an airport in the given year with the following atttributes:\n",
    "\n",
    "Variable   | Description\n",
    "-----------|---------------------------------------------------------------\n",
    "Rank       | Airport ranking by number of passengers for given year\n",
    "Airport    | Name of the airport\n",
    "Location   | Location of the airport\n",
    "Country    | Airport Country\n",
    "IATA Code  | Three-letter code designating the airport defined by the International Air Transport Association\n",
    "ICAO Code  | Four-letter code designating the airport as defined by the International Civil Aviation Organization\n",
    "Passengers | Total number of passengers for the given year\n",
    "Year       | Year of ranking\n",
    "\n",
    "**Run the cell below to import the needed modules and dataset.**\n",
    "\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "Airports = pd.read_csv('../Datasets/Airports.csv')\n",
    "Airports\n",
    "\n",
    "### Question 1.1 *(4 pts)*\n",
    "\n",
    "What is the total number of passengers in the top 50 most crowded airports in 2020? Assign `tot_pass_2020` to this value.\n",
    "\n",
    "sorted_airports = Airports.sort_values(\"Passengers\").head(10)\n",
    "sorted_airports[\"Passengers\"]\n",
    "# I dont need to filter for years because these top 10 are all in the year 2020\n",
    "\n",
    "tot_pass_2020 = sorted_airports[\"Passengers\"].sum()\n",
    "print(tot_pass_2020)\n",
    "\n",
    "### Question 1.2 *(4 pts)*\n",
    "\n",
    "What are the names of the airports with the largest number of passengers in each of the years? Assign `busiest_airports` to an array containing the names of these airports (possibly with repeated names of the same airport), in descending order of their passenger totals.\n",
    "\n",
    "Note: You may see **\\xa0** in the strings for the names of the airports. This is fine and due to the encoding of the data.\n",
    "\n",
    "Airports.info()\n",
    "\n",
    "# NOTE: THIS IS ASKING FOR THE MAX FROM EACH YEAR \n",
    "Airports[\"Year\"].unique()\n",
    "\n",
    "# NOTE: IN DESCENDING ORDER FROM PASSENGER TOTALS \n",
    "\n",
    "# 2016\n",
    "Airports_2016 = Airports[Airports[\"Year\"] == 2016] \n",
    "Airports_max_2016 = Airports_2016[\"Passengers\"].index.max()\n",
    "a2016 = Airports_2016.loc[[Airports_max_2016]]\n",
    "\n",
    "# 2017\n",
    "Airports_2017 = Airports[Airports[\"Year\"] == 2017] \n",
    "Airports_max_2017 = Airports_2017[\"Passengers\"].index.max()\n",
    "a2017 = Airports_2017.loc[[Airports_max_2017]]\n",
    "\n",
    "# 2018\n",
    "Airports_2018 = Airports[Airports[\"Year\"] == 2018] \n",
    "Airports_max_2018 = Airports_2018[\"Passengers\"].index.max()\n",
    "a2018 = Airports_2018.loc[[Airports_max_2018]]\n",
    "\n",
    "# 2019\n",
    "Airports_2019 = Airports[Airports[\"Year\"] == 2019] \n",
    "Airports_max_2019 = Airports_2019[\"Passengers\"].index.max()\n",
    "a2019 = Airports_2019.loc[[Airports_max_2019]]\n",
    "\n",
    "# 2020\n",
    "Airports_2020 = Airports[Airports[\"Year\"] == 2020] \n",
    "Airports_max_2020 = Airports_2020[\"Passengers\"].index.max()\n",
    "a2020 = Airports_2020.loc[[Airports_max_2020]]\n",
    "\n",
    "combined_frame = pd.concat([a2016,a2017,a2018,a2019,a2020])\n",
    "combined_frame_sorted = combined_frame.sort_values(\"Passengers\")\n",
    "\n",
    "check = make_array(combined_frame_sorted[\"Airport\"])\n",
    "check_2 = check[0]\n",
    "\n",
    "busiest_airports = check_2\n",
    "busiest_airports\n",
    "\n",
    "### Question 1.3 *(4 pts)*\n",
    "\n",
    "What is the average number of passengers in the top ranked airports for each of the years? Assign this value to `busiest_airports_average`.\n",
    "\n",
    "# print(combined_frame)\n",
    "\n",
    "# Average Number of Passengers for the 2016 Top Ranked Airport\n",
    "QA_2016 = Airports[Airports[\"Airport\"] == 'Qatar\\xa0Hamad International Airport'][\"Passengers\"].sum()\n",
    "QA_entries = len(Airports[Airports[\"Airport\"] == 'Qatar\\xa0Hamad International Airport'][\"Passengers\"])\n",
    "QA_2016 = QA_2016/QA_entries\n",
    "\n",
    "# Average Number of Passengers for the 2017 Top Ranked Airport\n",
    "RU_2017 = Airports[Airports[\"Airport\"] == 'Russia\\xa0Sheremetyevo International Airport'][\"Passengers\"].sum()\n",
    "RU_entries = len(Airports[Airports[\"Airport\"] == 'Russia\\xa0Sheremetyevo International Airport'][\"Passengers\"])\n",
    "RU_2017 = RU_2017/RU_entries\n",
    "\n",
    "# Average Number of Passengers for the 2018 Top Ranked Airport\n",
    "JA_2018 = Airports[Airports[\"Airport\"] == 'Japan\\xa0Narita International Airport'][\"Passengers\"].sum()\n",
    "JA_entries = len(Airports[Airports[\"Airport\"] == 'Japan\\xa0Narita International Airport'][\"Passengers\"])\n",
    "JA_2018 = JA_2018/JA_entries\n",
    "\n",
    "# Average Number of Passengers for the 2019 Top Ranked Airport\n",
    "\n",
    "# Average Number of Passengers for the 2020 Top Ranked Airport \n",
    "tot_US = Airports[Airports[\"Airport\"] == 'United States\\xa0Newark Liberty International Airport'][\"Passengers\"].sum()\n",
    "US_entires = len(Airports[Airports[\"Airport\"] == 'United States\\xa0Newark Liberty International Airport'][\"Passengers\"])\n",
    "US_2020 = tot_US/US_entires # The US Newark airports have an average of 38395597.4 passengers over the course of 5 years\n",
    "\n",
    "busiest_airport_average = {\"Top Ranked Year\":[2016,2017,2018,2019,2020],\n",
    "                          \"Airport Name\":[\"Qatar Hamad International Airport\", \"Russia Sheremetyevo International Airport\",\n",
    "                                         \"Japan Narita International Airport\", \"Japan Narita International Airport\", \n",
    "                                         \"United States Newark Liberty International Airport\"], \n",
    "                          \"Average Number of Passengers From 2016 to 2020\":[QA_2016, RU_2017, JA_2018, JA_2018, US_2020]}\n",
    "pd.DataFrame(busiest_airport_average)\n",
    "\n",
    "### Question 2.1 *(4 pts)*\n",
    "\n",
    "Construct a new table `Airports2` that contains each of the rows and columns of the `Airports` table and also contains a new column, `Passengers in Millions`, which is the number of passengers for each airport counted in millions of passengers.  \n",
    "\n",
    "Airports[\"Passengers in Millions\"] = Airports[\"Passengers\"]/1000000\n",
    "\n",
    "Airports2 = Airports\n",
    "print(Airports2.head(5))\n",
    "\n",
    "### Question 2.2 *(4 pts)*\n",
    "\n",
    "Use the `Airports2` table to construct a histogram for the numbers of passengers in the millions. Construct the histogram such that the bins begin at 10 million passengers, are 10 million passengers wide, and the histogram has bins containing all of the data from the airports in the table.\n",
    "\n",
    "# Make hist of number of passengers in the millions \n",
    "# Bin begins at 10 million passangers, are 10 passengers wide, and all the date from teh airports in the table \n",
    "Airports2[\"Passengers in Millions\"].describe()\n",
    "# Max is 110\n",
    "\n",
    "my_bins = np.arange(10, 121, 10) # This is because the max value is 110.5 and I need a bin higher than just 110 to contain it\n",
    "labels = [\"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-109\", \"100-109\", \"110-119\"]\n",
    "\n",
    "plt.hist(Airports2[\"Passengers in Millions\"], my_bins, edgecolor = \"black\")\n",
    "plt.xlabel(\"Millions of Passengers\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Millions of Passengers on Flights Between 2016 - 2020\")\n",
    "\n",
    "### Question 2.3 *(2 pts)*\n",
    "\n",
    "From the histogram constructed in the previous question, which binning of millions of passengers has the most airports in the data? Set `most_airports_bin` to an array containging two values, the lower and upper limits of that bin.\n",
    "\n",
    "most_airports_bin = make_array(40, 49)\n",
    "most_airports_bin\n",
    "\n",
    "### Question 2.4 *(4 pts)*\n",
    "\n",
    "How many airports are in the bin from the previous question? Assign this value to `num_airports`. Consider each row of the data as a unique airport, even though the same airport may be shown over multiple years. You should calculate this number from the `Airports2` table and not estimate the value from the histogram.\n",
    "\n",
    "Airports2[\"Bins\"] = pd.cut(Airports2[\"Passengers in Millions\"], my_bins, labels = labels)\n",
    "Airports2[\"Bins\"].isnull().sum() \n",
    "# One is null, I bet that is whatever is not gathered by the 10 \n",
    "Airports2[Airports2[\"Bins\"].isnull()].index.tolist()\n",
    "Airports2.loc[[50]] # Fixed previous error in previous question, this is now in a bin \n",
    "\n",
    "Airports2[\"Bins\"].value_counts()\n",
    "\n",
    "num_airports = Airports2[\"Bins\"].value_counts().max()\n",
    "num_airports\n",
    "\n",
    "### Question 3.1 *(8 pts)*\n",
    "\n",
    "Construct a bar chart showing the number of times each country appears in the top 50 airports for all years in the data. The bar chart should have the country with the highest number of airports at the top, and then the remaining countries in decreasing order.\n",
    "\n",
    "# The number of times each country appears in the top 50 airports for all years in the data \n",
    "Airports[\"Country\"].unique()\n",
    "countries = ['China', 'United States', 'Japan', 'India', 'United Arab Emirates',\n",
    "       'Turkey', 'France', 'United Kingdom', 'Mexico', 'Vietnam',\n",
    "       'South Korea', 'Netherlands', 'Brazil', 'Russia', 'Germany',\n",
    "       'Spain', 'Thailand', 'Hong Kong SAR, China', 'Singapore',\n",
    "       'Malaysia', 'Indonesia', 'Canada', 'Taiwan', 'Philippines',\n",
    "       'Australia', 'Italy', 'The Netherlands', 'Republic of Korea',\n",
    "       'Qatar']\n",
    "\n",
    "Airports[\"Rank\"].unique()\n",
    "# There are only 50 ranks, so just make a value count of the countries \n",
    "Airports[\"Country\"].value_counts()\n",
    "\n",
    "Airports[\"Country\"].value_counts().plot(kind = \"bar\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Number of Times a Country Appears in the Top 50 Years From 2016 to 2020\")\n",
    "\n",
    "### Question 3.2 *(8 pts)*\n",
    "\n",
    "You should see that China and the United States have many more airports in the data than other countries. Construct a table, `US_China_tot`, using only the airports in the United States and China (over all years) that has five rows, one for each year in the data, and columns for the year (in ascending order), the total number passengers in these airports in China for each year, and the total number passengers in these airports in the United States for each year.\n",
    "\n",
    "Note: Depending on how you go about this problem, you may receive a **VisibleDeprecationWarning**. This is fine and will not impact your table.\n",
    "\n",
    "Airports[\"Country\"].unique()\n",
    "\n",
    "# GOal: Make a table that has chin and the US's airports passangers for each year \n",
    "\n",
    "# First Filter the data for just US and China\n",
    "US = Airports[Airports[\"Country\"] == \"United States\"]\n",
    "China = Airports[Airports[\"Country\"] == \"China\"]\n",
    "US_China = Airports[(Airports[\"Country\"] == \"China\") | (Airports[\"Country\"] == \"United States\")]\n",
    "# US_China[\"Country\"].unique()\n",
    "# US_China_grouped = US_China.groupby(\"Country\")\n",
    "# US_China_grouped[\"Year\"].value_counts().plot(kind = \"barh\")\n",
    "\n",
    "# Make manually \n",
    "US_total_2016_pass = US[\"Passengers\"][US[\"Year\"] == 2016].sum()\n",
    "US_total_2017_pass = US[\"Passengers\"][US[\"Year\"] == 2017].sum()\n",
    "US_total_2018_pass = US[\"Passengers\"][US[\"Year\"] == 2018].sum()\n",
    "US_total_2019_pass = US[\"Passengers\"][US[\"Year\"] == 2019].sum()\n",
    "US_total_2020_pass = US[\"Passengers\"][US[\"Year\"] == 2020].sum()\n",
    "\n",
    "China_total_2016_pass = China[\"Passengers\"][China[\"Year\"] == 2016].sum()\n",
    "China_total_2017_pass = China[\"Passengers\"][China[\"Year\"] == 2017].sum()\n",
    "China_total_2018_pass = China[\"Passengers\"][China[\"Year\"] == 2018].sum()\n",
    "China_total_2019_pass = China[\"Passengers\"][China[\"Year\"] == 2019].sum()\n",
    "China_total_2020_pass = China[\"Passengers\"][China[\"Year\"] == 2020].sum()\n",
    "\n",
    "# After you get each put into a dictionary, then set index to country \n",
    "# Then format according to the problem\n",
    "# US_China_tot_part1 = {\"Country\":[\"United States\", \"China\"], \n",
    "#                       \"2016\": [US_total_2016_pass, China_total_2016_pass], \n",
    "#                       \"2017\":[US_total_2017_pass, China_total_2017_pass], \n",
    "#                       \"2018\":[US_total_2018_pass, China_total_2018_pass], \n",
    "#                       \"2019\":[US_total_2019_pass, China_total_2019_pass],\n",
    "#                       \"2020\":[US_total_2020_pass, China_total_2020_pass]}\n",
    "# US_China_Passengers = pd.DataFrame(US_China_tot_part1)\n",
    "# US_China_Passengers_t = US_China_Passengers.set_index(\"Country\")\n",
    "# US_China_Passengers_t.transpose()\n",
    "\n",
    "# If I wanted the chart to be a little more proper \n",
    "US_China_tot_part2 = {\"United States\":[US_total_2016_pass, US_total_2017_pass, US_total_2018_pass, \n",
    "                                      US_total_2019_pass, US_total_2020_pass], \n",
    "                     \"China\":[China_total_2016_pass, China_total_2017_pass, China_total_2018_pass,\n",
    "                             China_total_2019_pass, China_total_2020_pass], \n",
    "                     \"Year\":[2016,2017,2018,2019,2020]}\n",
    "US_China_Passengers_pt2 = pd.DataFrame(US_China_tot_part2)\n",
    "US_C_Pass = US_China_Passengers_pt2.set_index(\"Year\")\n",
    "US_C_Pass\n",
    "\n",
    "US_China_tot = US_C_Pass\n",
    "US_China_tot\n",
    "\n",
    "### Question 3.3 *(4 pts)*\n",
    "\n",
    "Using the `US_China_tot` table from the previous question, construct an appropriate plot that shows these totals for passengers over time for each of the two countries.\n",
    "\n",
    "US_China_tot.plot()\n",
    "plt.ylabel(\"Passengers\")\n",
    "plt.title(\"Passengers Over Time in the United States versus China\")\n",
    "\n",
    "### Question 4.1 *(8 pts)*\n",
    "\n",
    "Write a function, `airport_prop` with arguments for IATA Code, country, and year for a given airport in the data (in that order). The function should return the proportion of passengers in that country's airports (from these top 50 most crowded airports) that went through the specific airport with the given IATA Code in the given year. \n",
    "\n",
    "# Airports2\n",
    "\n",
    "#Plan:\n",
    "\n",
    "# def airport_prop(IATA_Code, country, year):\n",
    "    # I need the proportion of passengers in that country's airports (from the top 50)\n",
    "        # So this should be IATA_CODE(for specific year)/Total_passengers_for_country(for specific year) \n",
    "            # Return the final proportion \n",
    "\n",
    "def airport_prop(IATA_Code, Country, Year):\n",
    "    function_df = Airports2[Airports2[\"Year\"] == Year]\n",
    "    IATA_People = function_df[function_df[\"IATA Code\"] == str(IATA_Code)][\"Passengers\"].sum()\n",
    "    Total_People = function_df[function_df[\"Country\"] == str(Country)][\"Passengers\"].sum()\n",
    "    division_math = IATA_People/Total_People\n",
    "    return division_math\n",
    "\n",
    "Run the cell below to check your function.\n",
    "\n",
    "airport_prop('DFW','United States', 2019)\n",
    "\n",
    "### Question 4.2 *(6 pts)*\n",
    "\n",
    "Construct a new table, `Airports_prop_2020` for just the 2020 data. It should have all of the columns of the `Airports` table as well as a new column `Passengers Proportion`, which is the proportion of passengers from a given country's airports (from these top 50 most crowded airports) that went through a given airport in 2020. You should use the apply function and your function created in the previous question to answer this question. Sort the data in the `Airports_prop_2020` table by this new `Passengers Proportion` column in ascending order.\n",
    "\n",
    "# Airports2.head(1)\n",
    "\n",
    "# Step 1: Filter for just the 2020 data \n",
    "Airports_2020_Only = Airports2[Airports2[\"Year\"] == 2020]\n",
    "\n",
    "code = Airports_2020_Only[\"IATA Code\"]\n",
    "country = Airports_2020_Only[\"Country\"]\n",
    "year = Airports_2020_Only[\"Year\"]\n",
    "\n",
    "len(Airports_2020_Only)\n",
    "\n",
    "new_info = [] \n",
    "\n",
    "for x in np.arange(0,50,1):\n",
    "    new_info.append(airport_prop(code[x], country[x], year[x]))\n",
    "\n",
    "dictionary = {\"Passengers Proportion\": new_info}\n",
    "dict_df = pd.DataFrame(dictionary)\n",
    "\n",
    "array_attempt = make_array(new_info)\n",
    "Airports_2020_Only[\"Passengers Proportion\"] = new_info\n",
    "\n",
    "# Airports_2020_Only.sort_values(\"Passengers Proportion\")\n",
    "\n",
    "Airports_prop_2020 = Airports_2020_Only.sort_values(\"Passengers Proportion\")\n",
    "Airports_prop_2020.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dc46a",
   "metadata": {},
   "source": [
    "# STOR 120: Take Home Midterm 2\n",
    "\n",
    "60 points total\n",
    "\n",
    "**Due:** Wednesday, March 30th to Gradescope before the start of class time.\n",
    "\n",
    "    Section 001: 12:20pm\n",
    "    Section 002:  1:25pm\n",
    "  \n",
    "**Directions:** The exam is open book, notes, course materials, internet, and all things that are not direct communication with others. Just as with all course assignments, you will submit exams to Gradescope as Jupyter Notebooks with the ipynb file extension. To receive full credit, you should show all of your code used to answer each question. Make sure to view your submission in Gradescope and verify that it is the correct file and has the format that you intended it to have, including all code being shown and run.\n",
    "\n",
    "Come to office hours if you have specific questions regarding the exam. Due to the large class sizes, individual questions sent via email are not possible to answer for all students. Please refrain from posting public questions to Piazza before the exam is due.  \n",
    "\n",
    "**Run the cell below to import the needed modules**\n",
    "\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from collections import Counter\n",
    "from random import choices\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "## 1. The Birthday Paradox *(20 points total)*\n",
    "\n",
    "Suppose that you are at a party and there are exactly 23 people are in the room. What are the chances that at least 2 of the people share the same birthday (not considering the year)? Suprising to most, the probability is actually greater than 50%! This is refered to as the *Birthday Paradox*, although it's only a “paradox” because our brains can’t handle the compounding power of exponents. To investigate this \"paradox\", you are going to simulate it.\n",
    "\n",
    "**Question 1.1.** Construct an array `days` with 365 elements, numbered 1 through 365. We'll use this as our possible birthdays (not considering leap years). Then write a function `My_Party` that takes in one argument, the size of the party `n`, simulates a party of size `n` with `n` randomly selected birthdays, and returns a value of `1` if at least two people at the party have the same birthday. If no one at the party has the same birthday, the function should return `0`. *(7 pts)*\n",
    "\n",
    "days = np.arange(1,366, 1)\n",
    "days_list = []\n",
    "\n",
    "for x in np.arange(0, len(days),1): \n",
    "    days_list.append(days[x])\n",
    "    \n",
    "party_size = 23\n",
    "\n",
    "party_people_birthdays = np.random.choice(days_list, size=party_size, replace = True)\n",
    "\n",
    "party_people_birthdays\n",
    "\n",
    "party_people_birthdays_list = []\n",
    "\n",
    "# Make the array a list \n",
    "for y in np.arange(party_size):\n",
    "    party_people_birthdays_list.append(party_people_birthdays[y])\n",
    "    \n",
    "party_people_birthdays_list\n",
    "\n",
    "res = {}\n",
    "\n",
    "for i in party_people_birthdays_list:\n",
    "    res[i] = party_people_birthdays_list.count(i)\n",
    "    \n",
    "print(res)\n",
    "\n",
    "# If there is a key in this above dictionary that is equal to or greater than 2, then we want to add 1 to a counter \n",
    "\n",
    "count = 0 \n",
    "for key in res:\n",
    "    if res[key] >= 2: \n",
    "        count = 1\n",
    "    else: \n",
    "        count = count + 0 \n",
    "        \n",
    "count\n",
    "\n",
    "# Put it all together \n",
    "days = np.arange(1,366, 1)\n",
    "days_list = []\n",
    "\n",
    "for x in np.arange(0, len(days),1): \n",
    "    days_list.append(days[x])\n",
    "    \n",
    "def My_Party(party_size):\n",
    "    party_people_birthdays = np.random.choice(days_list, size=party_size, replace = True)\n",
    "    party_people_birthdays_list = []\n",
    "    \n",
    "    # Make the array a list \n",
    "    for y in np.arange(party_size):\n",
    "        party_people_birthdays_list.append(party_people_birthdays[y])\n",
    "    \n",
    "    res = {}\n",
    "    \n",
    "    for i in party_people_birthdays_list:\n",
    "        res[i] = party_people_birthdays_list.count(i)\n",
    "    \n",
    "    # print(res) \n",
    "    # Uncomment \"print(res)\" if you want to confirm things are working \n",
    "    \n",
    "    count = 0 \n",
    "    \n",
    "    for key in res:\n",
    "        if res[key] >= 2: \n",
    "            count = 1\n",
    "        else: \n",
    "            count = count + 0 \n",
    "    return(count)\n",
    "    \n",
    "# Do not delete or change the below line of code\n",
    "My_Party(23)\n",
    "\n",
    "**Question 1.2** Perform a simulation that simulates 10,000 parties of 23 people with randomly selected birthdays. In what proportion of these parties did at least two people have the same birthday? *(3 pts)*\n",
    "\n",
    "simulation_data = []\n",
    "simulations = 100000\n",
    "\n",
    "for x in np.arange(simulations):\n",
    "    simulation_data.append(My_Party(23))\n",
    "\n",
    "proportion_yes_2_same_bdays = simulation_data.count(1)/simulations\n",
    "proportion_yes_2_same_bdays\n",
    "\n",
    "**Question 1.3** Now let's see how we can calculate this mathematically for a smaller party size. Suppose that you are at a small get together with only 5 people. Calculate (using algebra and not a simulation) the probability of having a party of 5 people where at least two people have the same birthday. Assign this value (with algebra showing how you calculated this answer) to `Birthday_Paradox_for_5`. *(2 pts)*\n",
    "\n",
    "# The question is asking for us to NOT use the function, but to calculate \"by hand\" how to find the: \n",
    "# probability of a party of 5 people where at least 2 people have the same birthday \n",
    "\n",
    "# Probability that no one has the same birthday, excluding leap years \n",
    "no_same_bday = (365/365) * (364/365) * (363/365) * (362/365) * (361/365)\n",
    "\n",
    "# Probability at least 2 share bday \n",
    "at_least_2_bday = 1 - no_same_bday\n",
    "\n",
    "Birthday_Paradox_for_5 = at_least_2_bday\n",
    "Birthday_Paradox_for_5\n",
    "\n",
    "**Question 1.4** Reusing much of the code from question 1.2, write a function `Many_of_My_Parties` that performs a simulation that simulates 10,000 parties of size `n` people with randomly selected birthdays. The function should return the proportion of these parties where at least two people have the same birthday. *(4 pts)*\n",
    "\n",
    "def Many_of_My_Parties(party_size): \n",
    "    simulation_data = []\n",
    "    simulations = 100000\n",
    "\n",
    "    for x in np.arange(simulations):\n",
    "        simulation_data.append(My_Party(party_size))\n",
    "\n",
    "    proportion_yes_2_same_bdays = simulation_data.count(1)/simulations\n",
    "    return(proportion_yes_2_same_bdays)\n",
    "\n",
    "# Do not delete or change the below line of code\n",
    "Many_of_My_Parties(5)\n",
    "\n",
    "**Question 1.5** Construct a table `Birthday_Paradox_for_n` containing a column `Size of Party` for party sizes of 5, 10, 15, 20, 25, 30, 35, 40, and 45. A second column `At Least Two` should contain your simulated probabilities for parties of the `Size of the Party` having at least two people with the same birthday. *(4 pts)*\n",
    "\n",
    "Note: This code may take a few more seconds to run than our typical simulations\n",
    "\n",
    "Birthday_Paradox_for_n = Table().with_columns(\"Size of Party\", np.arange(5,46,5), \n",
    "                          \"At Least Two\", make_array(Many_of_My_Parties(5), Many_of_My_Parties(10), \n",
    "                                           Many_of_My_Parties(15), Many_of_My_Parties(20), \n",
    "                                           Many_of_My_Parties(25), Many_of_My_Parties(30),\n",
    "                                           Many_of_My_Parties(35), Many_of_My_Parties(40),\n",
    "                                           Many_of_My_Parties(45)))\n",
    "\n",
    "\n",
    "# Birthday_Paradox_for_n\n",
    "# The above thing shows the table, and it works\n",
    "\n",
    "# Do not delete or change the below line of code\n",
    "Birthday_Paradox_for_n.plot('Size of Party')\n",
    "\n",
    "## 2. Which Justin? *(40 points total)*\n",
    "\n",
    "The table `Which_Justin` contains a random sample of data collected from Instagram posts made by three famous Justins: Bieber, Trudeau, and Timberlake. Each row of the table is an Instagram post, with variables such as the number of likes, comments, hashtags, characters, words, emojis, and mentions. Run the cell below to load the dataset.\n",
    "\n",
    "Which_Justin_df = pd.read_csv('Which_Justin_120.csv')\n",
    "Which_Justin_df[\"username\"].unique()\n",
    "\n",
    "**Question 2.1** How similar or different do we expect the attribues of the posts by these Justins to be? You may believe that the average number of emojis used in Instagram posts by Justin Beiber is greater than that of Justin Trudeau. Let's perform a hypothesis test to test this claim with our sample data. For this test, we should use the hypotheses below. *(12 pts)*\n",
    "\n",
    "**Null Hypothesis:** The average number of emojis used in Justin Bieber's Instagram posts is equal to the average number of emojis used in Justin Trudeau's Instagram posts      \n",
    "    \n",
    "**Alternative Hypothesis**: The average number of emojis used in Justin Bieber's Instagram posts is greater than the average number of emojis used in Justin Trudeau's Instagram posts\n",
    "\n",
    "Perform a hypotheses test to test the above hypotheses. **To receive full credit you should:**\n",
    "\n",
    "1. Choose an appropriate test statistic\n",
    "\n",
    "2. Find the value of the observed test statistic\n",
    "\n",
    "3. Assume the null hypothesis is true and sample from the theoretical population under the null hypothesis and obtain the simulated test statistic. Repeat this 1000 times.\n",
    "\n",
    "4. Plot your simulated test statistics in a histogram along with the observed test statistic\n",
    "\n",
    "5. Calculate the p-value based off of your observed and simulated test statistics\n",
    "\n",
    "6. Use the p-value to draw a conclusion and explain the conclusion using simple, non-technical language in the context of the problem with complete sentences\n",
    "\n",
    "You may do this in as many lines as needed, and may add cells as well. You can (should) use functions that we have used in class, homework, labs, or from the text!\n",
    "\n",
    "## Notes\n",
    "**What I want:**  We want to run a difference in means between the average number of emojis used inJB's IG posts vs JT's IG posts \n",
    "\n",
    "### A/B Testing Steps: \n",
    "1. Define a null and alternate model \n",
    "\n",
    "**Null Hypothesis:** The average number of emojis used in Justin Bieber's Instagram posts is equal to the average number of emojis used in Justin Trudeau's Instagram posts      \n",
    "    \n",
    "**Alternative Hypothesis**: The average number of emojis used in Justin Bieber's Instagram posts is greater than the average number of emojis used in Justin Trudeau's Instagram posts\n",
    "\n",
    "2. Choose a test statistic (typically the difference in means between two categories)\n",
    "\n",
    "**What I want:**  We want to run a difference in means between the average number of emojis used inJB's IG posts vs JT's IG posts \n",
    "\n",
    "**Compare** \n",
    "\n",
    "    - (A): n_emojies in JB IG Posts\n",
    "    \n",
    "    - (B): n_emojies in JT IG Posts\n",
    "\n",
    "Is the difference due to chance alone? We are assuming there is no difference and we want to test if there is a difference. \n",
    "\n",
    "# Define a new smaller table:\n",
    "JB_JT_df = Which_Justin_df[(Which_Justin_df[\"username\"] == \"justinbieber\") | (Which_Justin_df[\"username\"] == \"justinpjtrudeau\")]\n",
    "smaller = JB_JT_df[[\"username\", \"n_emojis\"]]\n",
    "smaller_groups = smaller.groupby(\"username\").mean()\n",
    "smaller_groups\n",
    "\n",
    "# # Histogram of the overlay; emoji use for JB vs the emoji use for JT \n",
    "# smaller_groups.hist(by = \"username\")\n",
    "\n",
    "# # 2. Choose a test statistic (typically the difference in means between two categories\n",
    "# # Group(A) Average - Group(B) Average \n",
    "# # JB = A \n",
    "# # JT = B \n",
    "# # Negative values will tell me there is more evidence to support the null hypothesis \n",
    "# # Positive values will support the alternative hypothesis \n",
    "\n",
    "means_table = smaller_groups\n",
    "print(means_table)\n",
    "observed_difference = means_table[\"n_emojis\"][0] - means_table[\"n_emojis\"][1]\n",
    "observed_difference\n",
    "\n",
    "# 1. Choose a test statistic (typically the difference in means between two categories)\n",
    "\n",
    "## **What I want:**  We want to run a difference in means between the average number of emojis used inJB's IG posts vs JT's IG posts \n",
    "\n",
    "Which_Justin = Table.read_table('Which_Justin_120.csv')\n",
    "\n",
    "def difference_of_means(table, label, group_label):\n",
    "    \"\"\"Takes: name of table, column label of numerical variable,\n",
    "    column label of group-label variable\n",
    "    Returns: Difference of means of the two groups\"\"\"\n",
    "    \n",
    "    #table with the two relevant columns\n",
    "    reduced = table.select(label, group_label)  \n",
    "    \n",
    "    # table containing group means\n",
    "    means_table = reduced.group(group_label, np.average)\n",
    "    # array of group means\n",
    "    means = means_table.column(1)\n",
    "    \n",
    "    return means.item(1) - means.item(0)\n",
    "\n",
    "difference_of_means(Which_Justin, \"n_emojis\", \"username\")\n",
    "\n",
    "# 3. Shuffle the labels of the original sample, find your simulated test statistic, and repeat many times\n",
    "def one_simulated_difference(table, label, group_label):\n",
    "    \"\"\"Takes: name of table, column label of numerical variable,\n",
    "    column label of group-label variable\n",
    "    Returns: Difference of means of the two groups after shuffling labels\"\"\"\n",
    "    \n",
    "    # array of shuffled labels\n",
    "    shuffled_labels = table.sample(with_replacement = False).column(group_label)\n",
    "    \n",
    "    # table of numerical variable and shuffled labels\n",
    "    shuffled_table = table.select(label).with_column(\n",
    "        'Shuffled Label', shuffled_labels)\n",
    "    \n",
    "    return difference_of_means(shuffled_table, label, 'Shuffled Label')  \n",
    "\n",
    "one_simulated_difference(Which_Justin, \"n_emojis\", \"username\")\n",
    "\n",
    "differences = make_array()\n",
    "\n",
    "for i in np.arange(1000):\n",
    "    new_difference = one_simulated_difference(Which_Justin, \"n_emojis\", \"username\")\n",
    "    differences = np.append(differences, new_difference)\n",
    "\n",
    "# # 4. Find the value of the observed test statistic \n",
    "Table().with_column('Difference Between Group Means', differences).hist()\n",
    "print('Observed Difference:', observed_difference)\n",
    "\n",
    "plt.title('Prediction Under the Null Hypothesis');\n",
    "plt.plot([observed_difference, observed_difference], [0, .5], color='red', lw=2);\n",
    "\n",
    "# 5. Calculate the p-value based off your observed and simulated test statistics \n",
    "sum(differences <= observed_difference)/5000\n",
    "\n",
    "### 6. Use the p-value and p-value cutoff to draw a conclusion about the null hypothesis\n",
    "Based on the histogram, our observed value appears close to the center of the spread of the histogram. What we got here appears to be what we would expect to see by chance.  With a P-Value of 0.072 that is greater than a default alpha level of 0.05, which means we fail to reject the null hypothesis.  We have evidence to support that the average number of emojis used in JB's IG posts is equal to the average number of emokis used in JT's IG posts. \n",
    "\n",
    "**Question 2.2.1** Construct a new table `Justin_Timberlake` containing only the 200 Instagram posted made by Justin Timberlake in the `Which_Justin` table. The `Justin_Timberlake` table should have an additional column `hashtags`. The `n_hashtags` column is already contained in the table, counting the number of hashtags in each Instagram post. The new column `hashtags` should be equal to `1` if the post has at least 1 hashtag and equal to 0 if the post has no hashtags. *(4 pts)*\n",
    "\n",
    "Justin_Timberlake = Which_Justin_df[Which_Justin_df[\"username\"] == \"justintimberlake\"]\n",
    "Justin_Timberlake[\"hashtags\"] = [1 if x >= 1 else 0 for x in Justin_Timberlake[\"n_hashtags\"]]\n",
    "\n",
    "JT_1_hashtag = Justin_Timberlake[Justin_Timberlake[\"hashtags\"] == 1]\n",
    "JT_0_hashtag = Justin_Timberlake[Justin_Timberlake[\"hashtags\"] == 0]\n",
    "\n",
    "JT_0_hashtag\n",
    "\n",
    "**Question 2.2.2** Construct a 95% confidence interval to predict the average number of likes on Justin Timberlake's Instagram posts that contain at least one hashtag. *(6 pts)*\n",
    "\n",
    "**To receive full credit you should:**\n",
    "\n",
    "1. Take bootstrap samples from the original sample, compute the average number of likes, and repeat at least 1000 times\n",
    "\n",
    "2. Determine the upper and lower bounds of the 95% confidence interval\n",
    "\n",
    "3. Assign the bounds of the confidence interval to: lower_bound_222, upper_bound_222\n",
    "\n",
    "You may do this in as many lines as needed, and may add cells as well. You can (should) use functions that we have used in class, homework, labs, or from the text! You do **not** need to plot a histogram of the bootstrap statistics with the confidence interval.\n",
    "\n",
    "JT_table_1_hashtags = Table.from_df(JT_1_hashtag, keep_index=False)\n",
    "#1. Take bootstrap samples from the original sample, compute the average number of likes, and repeat at least 1000 times\n",
    "our_sample = JT_table_1_hashtags.sample(100, with_replacement=False)\n",
    "our_sample_like_average = np.mean(our_sample.column('n_likes'))\n",
    "\n",
    "def one_bootstrap_mean():\n",
    "    single_sample = our_sample.sample()\n",
    "    return np.mean(single_sample.column('n_likes'))\n",
    "\n",
    "one_bootstrap_mean()\n",
    "\n",
    "bootstrap_means = make_array()\n",
    "\n",
    "for i in np.arange(1000):\n",
    "    new_mean = one_bootstrap_mean()\n",
    "    bootstrap_means = np.append(bootstrap_means, new_mean)\n",
    "\n",
    "lower_bound_222 = percentile(2.5, bootstrap_means)\n",
    "upper_bound_222 = percentile(97.5, bootstrap_means)\n",
    "\n",
    "lower_bound_222, upper_bound_222\n",
    "\n",
    "**Question 2.2.3** Construct a 95% confidence interval to predict the average number of likes on Justin Timberlake's Instagram posts that contain **zero** hashtags. *(6 pts)*\n",
    "\n",
    "**To receive full credit you should:**\n",
    "\n",
    "1. Take bootstrap samples from the original sample, compute the average number of likes, and repeat at least 1000 times\n",
    "\n",
    "2. Determine the upper and lower bounds of the 95% confidence interval\n",
    "\n",
    "3. Assign the bounds of the confidence interval to: lower_bound_223, upper_bound_223\n",
    "\n",
    "You may do this in as many lines as needed, and may add cells as well. You can (should) use functions that we have used in class, homework, labs, or from the text! You do **not** need to plot a histogram of the bootstrap statistics with the confidence interval.\n",
    "\n",
    "JT_table_0_hashtags = Table.from_df(JT_0_hashtag, keep_index=False)\n",
    "#1. Take bootstrap samples from the original sample, compute the average number of likes, and repeat at least 1000 times\n",
    "our_sample = JT_table_0_hashtags.sample(70, with_replacement=False)\n",
    "our_sample_like_average = np.mean(our_sample.column('n_likes'))\n",
    "\n",
    "def one_bootstrap_mean():\n",
    "    single_sample = our_sample.sample()\n",
    "    return np.mean(single_sample.column('n_likes'))\n",
    "\n",
    "one_bootstrap_mean()\n",
    "\n",
    "bootstrap_means = make_array()\n",
    "\n",
    "for i in np.arange(1000):\n",
    "    new_mean = one_bootstrap_mean()\n",
    "    bootstrap_means = np.append(bootstrap_means, new_mean)\n",
    "\n",
    "lower_bound_223 = percentile(2.5, bootstrap_means)\n",
    "upper_bound_223 = percentile(97.5, bootstrap_means)\n",
    "\n",
    "lower_bound_223, upper_bound_223\n",
    "\n",
    "**Question 2.2.4** Do you have evidence to say that the average number of likes on Justin Timberlake's Instagram posts is different between posts with at least one hashtag versus the posts without a hashtag? Why or why not? Use your confidence intervals in the previous two questions to justify your answer. *(2 pts)*\n",
    "\n",
    "No, we do not have evidence ot say that the average number of likes on Justin Timberlake's IG posts are different between posts with at least one hashtag vs posts without a hashtag because the 95% confidence interval contains overlapping values and if the true population value falls in that overlapping value set, then there would be no difference between posts with at least one hashtag vs posts without a hashtag.  If the confidence intervals did not overlap, we would have evidence that there could be a difference between the groups. \n",
    "\n",
    "**Question 2.3** A recent study analyzing Instagram posts during the first quarter of 2021 found that 16.6% of all Instagram posts contained a video. For the `Which_Justin` table containing posts from all three Justins, is there evidence to suggest that these Justins post videos a different proportion of the time compared to other Instagram users (posting videos 16.6% of the time)? *(10 pts)*\n",
    "\n",
    "Perform a hypotheses test to test the above claim. **To receive full credit you should:**\n",
    "\n",
    "1. Choose and cite appropriate null and alternative hypotheses\n",
    "\n",
    "2. Choose an appropriate test statistic\n",
    "\n",
    "3. Find the value of the observed test statistic\n",
    "\n",
    "4. Assume the null hypothesis is true and sample from the theoretical population under the null hypothesis and obtain the simulated test statistic. Repeat this 1000 times.\n",
    "\n",
    "5. Plot your simulated test statistics in a histogram along with the observed test statistic\n",
    "\n",
    "6. Calculate the p-value based off of your observed and simulated test statistics\n",
    "\n",
    "7. Use the p-value to draw a conclusion and explain the conclusion using simple, non-technical language in the context of the problem with complete sentences\n",
    "\n",
    "You may do this in as many lines as needed, and may add cells as well. You can (should) use functions that we have used in class, homework, labs, or from the text!\n",
    "\n",
    "smaller_JT = Which_Justin_df[[\"username\", \"is_video\"]]\n",
    "proportions_smaller = smaller_JT.groupby(\"username\").sum()/137\n",
    "proportions_smaller\n",
    "\n",
    "## JUSTIN BIEBER \n",
    "\n",
    "# 1. Choose and cite appropriate null and alternative hypotheses\n",
    "\n",
    "# Null Hypothesis: The proportion of videos posted by Justin Bieber on his IG is equal to the proportion of all IG posts containing videos \n",
    "# Alternative Hypothesis: The proportion of  videos posted by Justin Bieber on his IG is not equal \n",
    "\n",
    "model_proportions = make_array(proportions_smaller[\"is_video\"][0], .166)\n",
    "\n",
    "# 2. Choose an appropriate test statistic\n",
    "# We want a test for proportions \n",
    "\n",
    "def statistic(expected_prop, actual_prop):\n",
    "    return 100*abs(expected_prop - actual_prop)\n",
    "\n",
    "# 3. Find the value of the observed test statistic\n",
    "observed_statistic = statistic(0.5, 18/24)\n",
    "\n",
    "# 4. Assume the null hypothesis is true and sample from the theoretical population under the null hypothesis and obtain the simulated test statistic. Repeat this 1000 times.\n",
    "num_simulations = 1000\n",
    "\n",
    "def simulation_and_statistic(model_proportions, expected_proportion_correct):\n",
    "       \n",
    "    simulation_proportion_correct = sample_proportions(24, model_proportions)\n",
    "    one_statistic = statistic(simulation_proportion_correct.item(0), expected_proportion_correct)\n",
    "    \n",
    "    return one_statistic\n",
    "\n",
    "simulated_statistics = make_array()\n",
    "\n",
    "for i in np.arange(num_simulations):\n",
    "    simulated_statistic = simulation_and_statistic(model_proportions, 0.5)\n",
    "    simulated_statistics = np.append(simulated_statistics, simulated_statistic)\n",
    "\n",
    "\n",
    "# 5. Plot your simulated test statistics in a histogram along with the observed test statistic\n",
    "Table().with_column('Simulated Statistics', simulated_statistics).hist(bins = np.arange(0, 50, 4.16))\n",
    "plt.plot([observed_statistic, observed_statistic], [0, .08], color='red', lw=2);\n",
    "\n",
    "# 6. Calculate the p-value based off of your observed and simulated test statistics\n",
    "print(\"p-value:  \", sum(simulated_statistics >= observed_statistic)/num_simulations)\n",
    "\n",
    "\n",
    "## 7. Use the p-value to draw a conclusion and explain the conclusion using simple, non-technical language in the context of the problem with complete sentences\n",
    "\n",
    "Based on the histogram, our observed value appeares close to the center of the spread of the histrogram.  What we got here appears to be what we would expect to see by chance.  With a p-value of 0.632 that is greater than the default alpha level of 0.05, this means we fail to reject the null hypothesis.  We have evidence to support that the proportion of videos posted by Justin Bieber on his IG is equal to the proportion of all IG posts containing videos.\n",
    "\n",
    "## JUSTIN CANADA \n",
    "\n",
    "# 1. Choose and cite appropriate null and alternative hypotheses\n",
    "\n",
    "# Null Hypothesis: The proportion of videos posted by JUSTIN CANADA  on his IG is equal to the proportion of all IG posts containing videos \n",
    "# Alternative Hypothesis: The proportion of  videos posted by JUSTIN CANADA  on his IG is not equal \n",
    "\n",
    "model_proportions = make_array(proportions_smaller[\"is_video\"][1], .166)\n",
    "\n",
    "# 2. Choose an appropriate test statistic\n",
    "# We want a test for proportions \n",
    "\n",
    "def statistic(expected_prop, actual_prop):\n",
    "    return 100*abs(expected_prop - actual_prop)\n",
    "\n",
    "# 3. Find the value of the observed test statistic\n",
    "observed_statistic = statistic(0.5, 18/24)\n",
    "\n",
    "# 4. Assume the null hypothesis is true and sample from the theoretical population under the null hypothesis and obtain the simulated test statistic. Repeat this 1000 times.\n",
    "num_simulations = 1000\n",
    "\n",
    "def simulation_and_statistic(model_proportions, expected_proportion_correct):\n",
    "       \n",
    "    simulation_proportion_correct = sample_proportions(24, model_proportions)\n",
    "    one_statistic = statistic(simulation_proportion_correct.item(0), expected_proportion_correct)\n",
    "    \n",
    "    return one_statistic\n",
    "\n",
    "simulated_statistics = make_array()\n",
    "\n",
    "for i in np.arange(num_simulations):\n",
    "    simulated_statistic = simulation_and_statistic(model_proportions, 0.5)\n",
    "    simulated_statistics = np.append(simulated_statistics, simulated_statistic)\n",
    "\n",
    "\n",
    "# 5. Plot your simulated test statistics in a histogram along with the observed test statistic\n",
    "Table().with_column('Simulated Statistics', simulated_statistics).hist(bins = np.arange(0, 50, 4.16))\n",
    "plt.plot([observed_statistic, observed_statistic], [0, .08], color='red', lw=2);\n",
    "\n",
    "# 6. Calculate the p-value based off of your observed and simulated test statistics\n",
    "print(\"p-value:  \", sum(simulated_statistics >= observed_statistic)/num_simulations)\n",
    "\n",
    "\n",
    "## 7. Use the p-value to draw a conclusion and explain the conclusion using simple, non-technical language in the context of the problem with complete sentences\n",
    "\n",
    "Based on the histogram, our observed value appeares close to the center of the spread of the histrogram.  What we got here appears to be what we would expect to see by chance.  With a p-value of 0.866 that is greater than the default alpha level of 0.05, this means we fail to reject the null hypothesis.  We have evidence to support that the proportion of videos posted by JUSTIN CANADA  on his IG is equal to the proportion of all IG posts containing videos. \n",
    "\n",
    "## JUSTIN TIMBERLAKE \n",
    "\n",
    "# 1. Choose and cite appropriate null and alternative hypotheses\n",
    "\n",
    "# Null Hypothesis: The proportion of videos posted by JUSTIN TIMBERLAKE  on his IG is equal to the proportion of all IG posts containing videos \n",
    "# Alternative Hypothesis: The proportion of  videos posted by JUSTIN TIMBERLAKE  on his IG is not equal \n",
    "\n",
    "model_proportions = make_array(proportions_smaller[\"is_video\"][2], .166)\n",
    "\n",
    "# 2. Choose an appropriate test statistic\n",
    "# We want a test for proportions \n",
    "\n",
    "def statistic(expected_prop, actual_prop):\n",
    "    return 100*abs(expected_prop - actual_prop)\n",
    "\n",
    "# 3. Find the value of the observed test statistic\n",
    "observed_statistic = statistic(0.5, 18/24)\n",
    "\n",
    "# 4. Assume the null hypothesis is true and sample from the theoretical population under the null hypothesis and obtain the simulated test statistic. Repeat this 1000 times.\n",
    "num_simulations = 1000\n",
    "\n",
    "def simulation_and_statistic(model_proportions, expected_proportion_correct):\n",
    "       \n",
    "    simulation_proportion_correct = sample_proportions(24, model_proportions)\n",
    "    one_statistic = statistic(simulation_proportion_correct.item(0), expected_proportion_correct)\n",
    "    \n",
    "    return one_statistic\n",
    "\n",
    "simulated_statistics = make_array()\n",
    "\n",
    "for i in np.arange(num_simulations):\n",
    "    simulated_statistic = simulation_and_statistic(model_proportions, 0.5)\n",
    "    simulated_statistics = np.append(simulated_statistics, simulated_statistic)\n",
    "\n",
    "\n",
    "# 5. Plot your simulated test statistics in a histogram along with the observed test statistic\n",
    "Table().with_column('Simulated Statistics', simulated_statistics).hist(bins = np.arange(0, 50, 4.16))\n",
    "plt.plot([observed_statistic, observed_statistic], [0, .08], color='red', lw=2);\n",
    "\n",
    "# 6. Calculate the p-value based off of your observed and simulated test statistics\n",
    "print(\"p-value:  \", sum(simulated_statistics >= observed_statistic)/num_simulations)\n",
    "\n",
    "\n",
    "## 7. Use the p-value to draw a conclusion and explain the conclusion using simple, non-technical language in the context of the problem with complete sentences\n",
    "\n",
    "Based on the histogram, our observed value appeares close to the center of the spread of the histrogram.  What we got here appears to be what we would expect to see by chance.  With a p-value of 0.05 that is only slightly greater than the default alpha level of 0.05, this means we fail to reject the null hypothesis.  We have evidence to support the proportion of videos posted by JUSTIN TIMBERLAKE  on his IG is equal to the proportion of all IG posts containing videos.\n",
    "\n",
    "Despite barely reaching the cut off for a 0.05 alpha level, I would feel more comfortable running this again a different alpha level to see if we get the same result.  This barely meets the requirements to fail to reject the null hypothesis.\n",
    "\n",
    "I've run this a couple more times, and the pvalue fluxuates between failing to reject the null and rejecting the null; because the pvalue is so low, I would probably go with saying that this data errs more towards what we would not expect to see by chance because it's a bit off from the center of the spread.  It's hard to tell, but I think I would go with rejecting the null hypothesis since the pvalue is so low in general.  Therefore, we have evidence to support the proportion of videos posted by JUSTIN TIMBERLAKE  on his IG is not equal to the proportion of all IG posts containing videos.\n",
    "\n",
    "To get a better idea of this, I would really want to look at if there are any outliers or abnormal data points that may be skewing the data - this wouldn't change the outcome of the hypothesis test, but it would better explain why Justin Timberlake's emojoi use may be a little different from the normal proportion. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6d28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
